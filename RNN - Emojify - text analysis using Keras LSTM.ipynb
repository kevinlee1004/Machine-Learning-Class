{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify - sensitivity analysis on text data\n",
    "use Keras LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klee\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading csv files from local drive\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read test and train data\n",
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading wordvector glove\n",
    "with open('data/glove.6B.50d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        curr_word = line[0]\n",
    "        words.add(curr_word)\n",
    "        word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "    i = 1\n",
    "    words_to_index = {}\n",
    "    index_to_words = {}\n",
    "    for w in sorted(words):\n",
    "        words_to_index[w] = i\n",
    "        index_to_words[i] = w\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier-V2: Using LSTMs in Keras: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare function\n",
    "\n",
    "def sentences_to_indices(X, words_to_index, max_len):\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = words_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j += 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Keras Model\n",
    "# Define sentence_indices as the input of the graph\n",
    "input_shape=(maxLen,)\n",
    "sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "## Create the embedding layer pretrained with GloVe Vectors\n",
    "## Prepare embedding matrix used for \n",
    "vocab_len = len(words_to_index) + 1                  # adding 1 to fit Keras embedding\n",
    "emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "# Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "# Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "for word, index in words_to_index.items():\n",
    "    emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "### Build Keras Embedding models    \n",
    "# Define Keras embedding layer with the correct output/input sizes. \n",
    "embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "# Build the embedding layer, it is required before setting the weights of the embedding layer. \n",
    "embedding_layer.build((None,))\n",
    "# Set the weights of the embedding layer to the embedding matrix. So, it is pre-trained.\n",
    "embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "### Build Keras LSTM models    \n",
    "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "X = LSTM(128, return_sequences=True)(embeddings)\n",
    "# Add dropout with a probability of 0.5\n",
    "X1 = Dropout(0.5)(X)\n",
    "# Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "# Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "X2 = LSTM(128, return_sequences=False)(X1)\n",
    "# Add dropout with a probability of 0.5\n",
    "X3 = Dropout(0.5)(X2)\n",
    "# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "X4 = Dense(5)(X3)\n",
    "# Add a softmax activation\n",
    "X5 = Activation('softmax')(X4)\n",
    "    \n",
    "# Create Model instance which converts sentence_indices into X.\n",
    "model = Model(inputs=sentence_indices, outputs=X5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'lstm_1/transpose_1:0' shape=(?, ?, 128) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile the model with gradient option\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the function\n",
    "\n",
    "def sentences_to_indices(X, words_to_index, max_len):\n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words.\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            X_indices[i, j] = words_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j += 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare the train data\n",
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxLen)\n",
    "Y_train_oh = np.eye(5)[Y_train.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 1.5997 - acc: 0.1818\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5303 - acc: 0.2727\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.4792 - acc: 0.3030\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4175 - acc: 0.3712\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.3220 - acc: 0.5000\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1949 - acc: 0.6061\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.0345 - acc: 0.6667\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9047 - acc: 0.6591\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.8984 - acc: 0.6818\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.8104 - acc: 0.6970\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.7109 - acc: 0.7652\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.7548 - acc: 0.6818\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.6277 - acc: 0.7652\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5203 - acc: 0.8333\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5192 - acc: 0.8030\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4255 - acc: 0.8106\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3678 - acc: 0.8788\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4011 - acc: 0.8561\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3673 - acc: 0.8485\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3036 - acc: 0.8788\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3356 - acc: 0.8864\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2478 - acc: 0.9091\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1881 - acc: 0.9470\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2249 - acc: 0.9242\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1890 - acc: 0.9318\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1491 - acc: 0.9470\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1598 - acc: 0.9545\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1316 - acc: 0.9545\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1866 - acc: 0.9242\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2615 - acc: 0.9167\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3075 - acc: 0.9242\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1284 - acc: 0.9394\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1277 - acc: 0.9545\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1122 - acc: 0.9545\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0463 - acc: 0.9924\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0711 - acc: 0.9773\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0559 - acc: 0.9773\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.0445 - acc: 0.9848\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0315 - acc: 0.9924\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3395 - acc: 0.9242\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4185 - acc: 0.8636\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5620 - acc: 0.8788\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4577 - acc: 0.8712\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3296 - acc: 0.8712\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3060 - acc: 0.8939\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1597 - acc: 0.9470\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1864 - acc: 0.9545\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1162 - acc: 0.9621\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0882 - acc: 0.9924\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0773 - acc: 0.9848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21820e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Train the model\n",
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 4ms/step\n",
      "\n",
      "Test accuracy =  0.8214285799435207\n"
     ]
    }
   ],
   "source": [
    "## Prepare test data\n",
    "X_test_indices = sentences_to_indices(X_test, words_to_index, max_len = maxLen)\n",
    "Y_test_oh = np.eye(5)[Y_test.reshape(-1)]\n",
    "## Evealuate model with test data\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}\n",
    "\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòÑ prediction: he got a very nice raise\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: she got me a nice present\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is hard\tüòÑ\n",
      "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: work is horrible\tüòÑ\n",
      "Expected emoji:üç¥ prediction: any suggestions for dinner\tüòÑ\n",
      "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully\tüòÑ\n",
      "Expected emoji:üòû prediction: My life is so boring\t‚ù§Ô∏è\n",
      "Expected emoji:üòÑ prediction: will you be my valentine\t‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "### Find out mislabelled examples\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòû\n"
     ]
    }
   ],
   "source": [
    "### Predict with test data of not feeling happy\n",
    "x_test1 = np.array(['not feeling happy'])\n",
    "X_test_indices1 = sentences_to_indices(x_test1, words_to_index, maxLen)\n",
    "print(x_test1[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
